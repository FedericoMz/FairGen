{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from functools import partial\n",
    "from statistics import mean, stdev\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from math import dist\n",
    "from itertools import product\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statistics import mean, stdev\n",
    "\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import NearestNeighbors as NN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d81b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as ltb\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f176b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7bf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation import evaluate as eva\n",
    "\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from math import dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranker(df, attributes, class_name):\n",
    "    X = df[attributes].values\n",
    "    y = df[class_name].to_list()\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    X_proba = [None] * len(X)\n",
    "    probability = []\n",
    "    for k in range(len(X)):\n",
    "        X_proba[k] = np.append(X[k], (y[k], clf.predict_proba(X)[k][1]))\n",
    "\n",
    "    X_proba = np.array(X_proba, dtype=float).tolist()\n",
    "    \n",
    "    X_proba_list = []\n",
    "    \n",
    "    for arr in (X_proba):\n",
    "        X_proba_list.append(arr[:len(attributes)+2])\n",
    "        \n",
    "    X_proba = pd.DataFrame.from_records(X_proba_list)\n",
    "    X_proba.columns = attributes + [class_name] + ['proba']\n",
    "    \n",
    "    return X_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad216787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index (attribute, attributes):\n",
    "    for i in range(len(attributes)):\n",
    "        if attributes[i] == attribute:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_value (val, value_list):\n",
    "    \n",
    "    return value_list[np.argmin(np.abs(np.array(value_list)-val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_causal_regressor (data, index_X, index_y):\n",
    "    \n",
    "    X = data.iloc[:, index_X].values\n",
    "    y = data.iloc[:, [index_y]].values\n",
    "\n",
    "    regr = ltb.LGBMRegressor().fit(X, y)\n",
    "\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53520c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_causal_classifier (data, index_X, index_y):\n",
    "\n",
    "    X = data.iloc[:, index_X].values\n",
    "    y = data.iloc[:, [index_y]].values\n",
    "\n",
    "    classifier = ltb.LGBMClassifier().fit(X, y)    \n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb07ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrimination (df, sensitive_attributes, class_name):\n",
    "\n",
    "# ASSUMPTION: Each value of the attribute is discriminated\n",
    "# For each value, we therefore apply the Preferential Sampling formulas to compute the discrimination \n",
    "# If discrimination > 0, the assumption holds true\n",
    "# Otherwhise, it doesn't. This means the value is actually *privileged*\n",
    "# A dictionary of sensitive attributes and values is created as such\n",
    "#\n",
    "# Please note the sum of records to add / remove for each priviliged value\n",
    "# should be equal to the sum of records to add / removed for each discriminated value\n",
    "#\n",
    "# A rounding error is possible\n",
    "\n",
    "    \n",
    "    sensitive_dict = {}\n",
    "    \n",
    "    tot_disc = 0\n",
    "    tot_pos = 0\n",
    "    \n",
    "    #df = X_proba\n",
    "    \n",
    "    for attr in sensitive_attributes:\n",
    "        print ()\n",
    "        print (\"Analizing\", attr, \"...\")\n",
    "        sensitive_dict[attr] = {}\n",
    "        sensitive_dict[attr]['D'] = {}\n",
    "        sensitive_dict[attr]['P'] = {}\n",
    "        sensitive_dict[attr]['D']['values_list'] = []\n",
    "        sensitive_dict[attr]['P']['values_list'] = []\n",
    "        values = df[attr].unique()\n",
    "        for val in values:\n",
    "            PP = df[(df[attr] != val) & (df[class_name] == 1)].values.tolist()\n",
    "            PN = df[(df[attr] != val) & (df[class_name] == 0)].values.tolist()\n",
    "            DP = df[(df[attr] == val) & (df[class_name] == 1)].values.tolist()\n",
    "            DN = df[(df[attr] == val) & (df[class_name] == 0)].values.tolist()\n",
    "\n",
    "            disc = len(DN) + len(DP)\n",
    "            priv = len(PN) + len(PP)\n",
    "            pos = len(PP) + len(DP)\n",
    "            neg = len(PN) + len(DN)\n",
    "            \n",
    "            DP_exp = round(disc * pos / len(df))\n",
    "            PP_exp = round(priv * pos / len(df))\n",
    "            DN_exp = round(disc * neg / len(df))\n",
    "            PN_exp = round(priv * neg / len(df))\n",
    "            \n",
    "            discrimination = len(PP) / (len(PP) + len(PN)) - len(DP) / (len(DP) + len(DN))\n",
    "       \n",
    "            if discrimination >= 0:\n",
    "                status = 'D'\n",
    "                sensitive_dict[attr][status][val] = {}\n",
    "                print(\"\")\n",
    "                print(val, \"is discriminated:\", discrimination)\n",
    "                \n",
    "                sensitive_dict[attr][status][val]['P'] = sorted(DP, key=lambda x:x[len(DP[0])-1])\n",
    "                sensitive_dict[attr][status][val]['P_exp'] = DP_exp\n",
    "                sensitive_dict[attr][status][val]['P_curr'] = 0\n",
    "                \n",
    "                for i in range(len(sensitive_dict[attr][status][val]['P'])):\n",
    "                    del sensitive_dict[attr][status][val]['P'][i][-1]\n",
    "                                \n",
    "                sensitive_dict[attr][status][val]['N'] = sorted(DN, key=lambda x:x[len(DN[0])-1], reverse = True)\n",
    "                sensitive_dict[attr][status][val]['N_exp'] = DN_exp\n",
    "                sensitive_dict[attr][status][val]['N_curr'] = 0\n",
    "\n",
    "                for i in range(len(sensitive_dict[attr][status][val]['N'])):\n",
    "                    del sensitive_dict[attr][status][val]['N'][i][-1]\n",
    "                    \n",
    "                print(\"- DP:\", len(sensitive_dict[attr][status][val]['P']), '路 Expected:', DP_exp, \n",
    "                      '路 To be added:', abs(len(DP) - DP_exp))\n",
    "                print(\"- DN:\", len(sensitive_dict[attr][status][val]['N']), '路 Expected:', DN_exp, \n",
    "                      '路 To be removed:', abs(len(DN) - DN_exp))\n",
    "                \n",
    "                tot_disc = tot_disc + abs(len(DP) - DP_exp)\n",
    "                \n",
    "            else:\n",
    "                status = 'P'\n",
    "                sensitive_dict[attr][status][val] = {}\n",
    "                print(\"\")\n",
    "                print(val, \"is privileged:\", discrimination)   \n",
    "                \n",
    "                sensitive_dict[attr][status][val]['P'] = sorted(DP, key=lambda x:x[len(DP[0])-1])\n",
    "                sensitive_dict[attr][status][val]['P_exp'] = DP_exp\n",
    "                sensitive_dict[attr][status][val]['P_curr'] = 0\n",
    "\n",
    "                for i in range(len(sensitive_dict[attr][status][val]['P'])):\n",
    "                    del sensitive_dict[attr][status][val]['P'][i][-1]\n",
    "                    \n",
    "                sensitive_dict[attr][status][val]['N'] = sorted(DN, key=lambda x:x[len(DN[0])-1], reverse = True)\n",
    "                sensitive_dict[attr][status][val]['N_exp'] = DN_exp\n",
    "                sensitive_dict[attr][status][val]['N_curr'] = 0\n",
    "\n",
    "                for i in range(len(sensitive_dict[attr][status][val]['N'])):\n",
    "                    del sensitive_dict[attr][status][val]['N'][i][-1]\n",
    "                \n",
    "                print(\"- PP:\", len(sensitive_dict[attr][status][val]['P']), '路 Expected:', DP_exp, \n",
    "                      '路 To be removed:', abs(len(DP) - DP_exp))\n",
    "                print(\"- PN:\", len(sensitive_dict[attr][status][val]['N']), '路 Expected:', DN_exp, \n",
    "                      '路 To be added:', abs(len(DN) - DN_exp))\n",
    "                \n",
    "                tot_pos = tot_pos + abs(len(DP) - DP_exp)\n",
    "            \n",
    "            sensitive_dict[attr][status]['values_list'].append(val)\n",
    "        \n",
    "    round_error = abs(tot_disc - tot_pos)\n",
    "    \n",
    "    if round_error > 0:\n",
    "        print (\"\")\n",
    "        print (\"Due to a rounding error, the final dataset might be slightly smaller\")\n",
    "                                   \n",
    "    return sensitive_dict            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_individual(values, const, \n",
    "                      values_in_dataset_indexes, discrete_indexes, regular_indexes, \n",
    "                      causal_reg, causal_class, ds):\n",
    "    \n",
    "    df = pd.DataFrame(values)\n",
    "    \n",
    "    ind = [None] * (len(df.columns)-1)\n",
    "        \n",
    "    for i in regular_indexes:\n",
    "        val = df.iloc[:, i].to_list()\n",
    "        \n",
    "        if i in values_in_dataset_indexes: #if the feat can only assume values already in the dataset\n",
    "            if ds == 'Random': #if values are picked randomly\n",
    "                ind[i] = random.choice(list(set(val)))\n",
    "            elif ds == 'Fixed': #if values are picked w.r.t. their frequency\n",
    "                ind[i] = random.choice(val)\n",
    "        elif i in discrete_indexes: #if the feat can only assume a random value in a int range\n",
    "            ind[i] = random.randint(min(val), max(val))            \n",
    "        else: #if the feat can assume a float value in a range\n",
    "            ind[i] = random.uniform(min(val), max(val))\n",
    "            \n",
    "    for tup in const:\n",
    "        ind[tup[0]] = tup[1]\n",
    "    \n",
    "    for e in causal_reg + causal_class:\n",
    "        \n",
    "        X_indexes = e[0]\n",
    "        y = e[1]\n",
    "        pred = e[2]\n",
    "        \n",
    "        X_val = []\n",
    "        for index in X_indexes:\n",
    "            X_val.append(ind[index])\n",
    "             \n",
    "        predicted = pred.predict([X_val])\n",
    "        \n",
    "        if y in values_in_dataset_indexes:\n",
    "            value_list = df.iloc[:, y].to_list()\n",
    "            ind[y] = get_closest_value(predicted[0], value_list)        \n",
    "        elif y in discrete_indexes:\n",
    "            ind[y] = int(predicted[0])        \n",
    "        else: \n",
    "            ind[y] = predicted[0]\n",
    "    \n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1db38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6376dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(individual, forest, medoid, mode):\n",
    " \n",
    "    individual = individual[0]\n",
    "        \n",
    "    #print (medoid)\n",
    "    #print (\"IND:\", individual)\n",
    "    if mode == \"Outlier\":\n",
    "        score = float(forest.predict_proba(np.array([individual]))[0][1])\n",
    "    elif mode == \"Distance\":\n",
    "        if None in medoid:\n",
    "            score = float(forest.predict_proba(np.array([individual]))[0][1])\n",
    "        else:\n",
    "            score = distance.cosine(individual, medoid) / 2\n",
    "    elif mode == 'Hybrid':\n",
    "        score = float(forest.predict_proba(np.array([individual]))[0][1])\n",
    "        if None not in medoid:\n",
    "            score = (score + distance.cosine(individual, medoid) / 2) / 2\n",
    "    \n",
    "    return score,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mate(ind1, ind2, values, \n",
    "         values_in_dataset_indexes, discrete_indexes, regular_indexes, \n",
    "         causal_reg, causal_class):\n",
    "    \n",
    "    #custom crossover\n",
    "    \n",
    "    indpb = 0.34\n",
    "    \n",
    "    for i in regular_indexes:\n",
    "        if random.random() < indpb:\n",
    "            ind1[i], ind2[i] = ind2[i], ind1[i]\n",
    "            for e in causal_reg + causal_class:\n",
    "                if i in e[0]:\n",
    "                    df = pd.DataFrame(values)\n",
    "                    \n",
    "                    X_indexes = e[0]\n",
    "                    y = e[1]\n",
    "                    pred = e[2]\n",
    "\n",
    "                    X_val1 = []\n",
    "                    X_val2 = []\n",
    "                    for index in X_indexes:\n",
    "                        X_val1.append(ind1[index])\n",
    "                        X_val2.append(ind2[index])\n",
    "\n",
    "                    #ind1\n",
    "                    predicted1 = pred.predict([X_val1])\n",
    "\n",
    "                    if y in values_in_dataset_indexes:\n",
    "                        value_list = df.iloc[:, y].to_list()\n",
    "                        ind1[y] = get_closest_value(predicted1[0], value_list)        \n",
    "                    elif y in discrete_indexes:\n",
    "                        ind1[y] = int(predicted1[0])        \n",
    "                    else: \n",
    "                        ind1[y] = predicted1[0]\n",
    "                        \n",
    "                    #ind2\n",
    "                    predicted2 = pred.predict([X_val2])\n",
    "\n",
    "                    if y in values_in_dataset_indexes:\n",
    "                        value_list = df.iloc[:, y].to_list()\n",
    "                        ind2[y] = get_closest_value(predicted2[0], value_list)        \n",
    "                    elif y in discrete_indexes:\n",
    "                        ind2[y] = int(predicted2[0])        \n",
    "                    else: \n",
    "                        ind2[y] = predicted2[0]\n",
    "                        \n",
    "                    \n",
    "                    #print (\"CHILD1:\", ind1)\n",
    "                    #print (\"X:\", X_indexes, X_val1, \"y:\", y, predicted1, predicted1[0][0], ind1[y])\n",
    "                    #print (\"CHILD2:\", ind2)\n",
    "                    #print (\"X:\", X_indexes, X_val2, \"y:\", y, predicted2, predicted2[0][0], ind2[y])                    \n",
    "                \n",
    "    return ind1, ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(individual, values, \n",
    "           values_in_dataset_indexes, discrete_indexes, regular_indexes, \n",
    "           causal_reg, causal_class, ds):\n",
    "    \n",
    "#custom mutation\n",
    "\n",
    "    df = pd.DataFrame(values)\n",
    "    \n",
    "    i = random.choice(regular_indexes) #we select a random feature to mutate\n",
    "    \n",
    "\n",
    "    if i in values_in_dataset_indexes: \n",
    "        val = df.iloc[:, i].to_list()\n",
    "        \n",
    "        if ds == \"Random\":\n",
    "            individual[i] = random.choice(list(set(val)))\n",
    "        elif ds == \"Fixed\":\n",
    "            individual[i] = random.choice(val)\n",
    "    elif i in discrete_indexes: #if the feat can only assume a random value in a int range\n",
    "        val = [x for x in df.iloc[:, i].to_list() if x != individual[i]]\n",
    "        individual[i] = random.randint(min(val), max(val))                        \n",
    "    else: #if the feat can assume a float value in a range\n",
    "        val = [x for x in df.iloc[:, i].to_list() if x != individual[i]]\n",
    "        individual[i] = random.uniform(min(val), max(val))\n",
    "\n",
    "    \n",
    "    for e in causal_reg + causal_class: \n",
    "        if i in e[0]:\n",
    "            #print (\"Independent value mutated...\")\n",
    "\n",
    "            X_indexes = e[0]\n",
    "            y = e[1]\n",
    "            pred = e[2]\n",
    "\n",
    "            X_val = []\n",
    "            for index in X_indexes:\n",
    "                X_val.append(individual[index])\n",
    "\n",
    "            predicted = pred.predict([X_val])\n",
    "\n",
    "            if y in values_in_dataset_indexes:\n",
    "                value_list = df.iloc[:, y].to_list()\n",
    "                individual[y] = get_closest_value(predicted[0], value_list)        \n",
    "            elif y in discrete_indexes:\n",
    "                individual[y] = int(predicted[0])        \n",
    "            else: \n",
    "                individual[y] = predicted[0]\n",
    "            \n",
    "            #print (\"X:\", X_indexes, X_val, \"y:\", y, predicted, predicted[0][0], individual[y])    \n",
    "\n",
    "    #print (\"NEW MUTATED:\", individual)\n",
    "\n",
    "    return individual,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA(values, const, n_HOF, forest, medoid, \n",
    "       values_in_dataset_indexes, discrete_indexes, regular_indexes, \n",
    "       causal_reg, causal_class, mode, ds):\n",
    "    \n",
    "    print (\"GA started,\", n_HOF, \"individual(s) will be generated\")\n",
    "        \n",
    "    NUM_GENERATIONS = 50\n",
    "    POPULATION_SIZE = 150\n",
    "    \n",
    "    CXPB, MUTPB = 0.5, 0.15\n",
    "    \n",
    "    creator.create('Fitness', base.Fitness, weights=(-1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.Fitness)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    toolbox.register(\"random_individual\", random_individual, values, const, values_in_dataset_indexes, \n",
    "                     discrete_indexes, regular_indexes, causal_reg, causal_class, ds=ds)\n",
    "\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n",
    "                     toolbox.random_individual, n=1)\n",
    "\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    toolbox.register(\"evaluate\", evaluate, forest=forest, medoid=medoid, mode=mode)\n",
    "        \n",
    "    toolbox.register(\"mate\", mate, values=values, values_in_dataset_indexes=values_in_dataset_indexes, \n",
    "                     discrete_indexes=discrete_indexes, regular_indexes=regular_indexes, \n",
    "                     causal_reg=causal_reg, causal_class=causal_class)\n",
    "    \n",
    "    toolbox.register(\"mutate\", mutate, values=values, values_in_dataset_indexes=values_in_dataset_indexes, \n",
    "                     discrete_indexes=discrete_indexes, regular_indexes=regular_indexes, \n",
    "                     causal_reg=causal_reg, causal_class=causal_class, ds=ds)    \n",
    "    \n",
    "    toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "\n",
    "\n",
    "    pop = toolbox.population(n=POPULATION_SIZE)\n",
    "\n",
    "    hof = tools.HallOfFame(n_HOF)\n",
    "    #hof = tools.ParetoFront()\n",
    "    \n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)   \n",
    "    #stats.register('max', np.max, axis = 0)\n",
    "    stats.register('min', np.min) #, axis = 0)\n",
    "    stats.register('avg', np.mean) #, axis = 0)\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + stats.fields\n",
    "    \n",
    "    invalid_individuals = [ind for ind in pop if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_individuals)\n",
    "    for ind, fit in zip(invalid_individuals, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "        \n",
    "    hof.update(pop)\n",
    "    hof_size = len(hof.items)\n",
    "\n",
    "    record = stats.compile(pop)\n",
    "    logbook.record(gen=0, best=\"-\", nevals=len(invalid_individuals), **record)\n",
    "    print(logbook.stream)\n",
    "\n",
    "    \n",
    "    for gen in range(1, NUM_GENERATIONS + 1):\n",
    "        \n",
    "                # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        \n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1[0], child2[0])\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant[0])\n",
    "                del mutant.fitness.values\n",
    "            \n",
    "                \n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "    \n",
    "        # Update the hall of fame with the generated individuals\n",
    "        hof.update(offspring)\n",
    "        \n",
    "        # Replace the current population by the offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(pop) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        print(logbook.stream)\n",
    "        \n",
    "        \n",
    "    hof.update(pop)\n",
    "    \n",
    "    counter = 0\n",
    "    for e in hof.items:\n",
    "        counter = counter + 1\n",
    "        print(\"\")\n",
    "        print(\"#\"+str(counter))\n",
    "        print(\"Constraints:\", const)\n",
    "        print('Individual:', e)\n",
    "        print('Fitness:', e.fitness)\n",
    "\n",
    "\n",
    "    plt.figure(1)\n",
    "\n",
    "    # plot genetic flow statistics:\n",
    "    minFitnessValues, meanFitnessValues = logbook.select(\"min\", \"avg\")\n",
    "    plt.figure(2)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue')\n",
    "    plt.plot(meanFitnessValues, color='green')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Fitness Value')\n",
    "    plt.title('Avg and Min Fitness')\n",
    "    # show both plots:\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return hof.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86adb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596dc1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main (df, sensitive_attributes, class_name, causal_reg, causal_class,\n",
    "                           discrete_attributes, values_in_dataset_attributes, mode, ds):\n",
    "    print (\"FairGen is running...\")\n",
    "    print (\"\")\n",
    "    print (\"Target variable:\", class_name)\n",
    "    print (\"Sensitive attributes:\", sensitive_attributes)\n",
    "    print (\"\")\n",
    "    print (\"Discrete attributes:\", discrete_attributes)\n",
    "    print (\"Attributes with only dataset values:\", values_in_dataset_attributes)\n",
    "    print (\"\")\n",
    "    print (\"Regressor:\", causal_reg)\n",
    "    print (\"Classifier:\", causal_class)\n",
    "    \n",
    "    genetic_data = []\n",
    "        \n",
    "    values_in_dataset_indexes = []\n",
    "    \n",
    "    discrete_indexes = []\n",
    "    \n",
    "    regular_indexes = []\n",
    "\n",
    "    causal_reg_attributes = []\n",
    "    \n",
    "    causal_class_attributes = []\n",
    "            \n",
    "    attributes = [col for col in df.columns if col != class_name]\n",
    "    \n",
    "    \n",
    "    val_comb = []\n",
    "    for att in sensitive_attributes:\n",
    "        val_comb.append(list(df[att].unique()))\n",
    "    \n",
    "    df_combinations = list(product(*val_comb))\n",
    "    \n",
    "    print (\"Ranking the data...\")\n",
    "    \n",
    "    X_proba = ranker(df, attributes, class_name)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    edges = []\n",
    "    \n",
    "    if len(causal_reg) > 0:\n",
    "         \n",
    "        for e in causal_reg:\n",
    "            \n",
    "            X_index = []\n",
    "            for feat in e[0]:\n",
    "                X_index.append(get_index(feat, attributes))\n",
    "                edges.append((feat, e[1]))\n",
    "            y_index = get_index(e[1], attributes)\n",
    "            causal_reg_attributes.append(e[1])\n",
    "            \n",
    "            regressor = get_causal_regressor (X_proba, X_index, y_index)\n",
    "\n",
    "            e[0] = X_index\n",
    "            e[1] = y_index\n",
    "            e.append(regressor)\n",
    "            \n",
    "            # Each e in causal has:\n",
    "            # e[0] = name of ind variable\n",
    "            # e[1] = name of dep variable\n",
    "            # we turned e[0] and e[1] into the variable indexes and we added\n",
    "            # e[2] = predictor\n",
    "    \n",
    "    if len(causal_class) > 0:\n",
    "\n",
    "        for e in causal_class:\n",
    "\n",
    "            X_index = []\n",
    "            for feat in e[0]:\n",
    "                X_index.append(get_index(feat, attributes))\n",
    "                edges.append((feat, e[1]))\n",
    "            y_index = get_index(e[1], attributes)\n",
    "            causal_class_attributes.append(e[1])\n",
    "\n",
    "            classifier = get_causal_classifier (X_proba, X_index, y_index)\n",
    "\n",
    "            e[0] = X_index\n",
    "            e[1] = y_index\n",
    "            e.append(classifier)       \n",
    "    \n",
    "    nodes = df.columns\n",
    "   \n",
    "    dag = nx.DiGraph(edges)\n",
    "\n",
    "    dag.add_nodes_from(nodes)\n",
    "\n",
    "    nx.draw_networkx(dag, pos = nx.circular_layout(dag), font_size=10, node_size=350, node_color='#abdbe3')\n",
    "\n",
    "    plt.title('Assumed Ground Truth', fontsize=13)\n",
    "\n",
    "    plt.show()\n",
    "            \n",
    "    values = X_proba.copy()\n",
    "    \n",
    "    target = None\n",
    "     \n",
    "    #PART 1 - Discrimination Test\n",
    "    \n",
    "    print (\"Creating the sensitive dictionary...\")\n",
    "    \n",
    "    sensitive_dict = get_discrimination (X_proba, sensitive_attributes, class_name)\n",
    "    \n",
    "    X_proba = X_proba.iloc[:, :-1] #removing the last column (with the ranker proba)\n",
    "    \n",
    "    og_df = X_proba.copy() \n",
    "\n",
    "    \n",
    "    #Training outlier detection methods for fitness\n",
    "    \n",
    "    forest=IForest()\n",
    "\n",
    "    forest.fit(X_proba)\n",
    "        \n",
    "    X_proba = X_proba.values\n",
    "            \n",
    "    for att in discrete_attributes:\n",
    "        discrete_indexes.append(get_index(att, attributes)) \n",
    "\n",
    "    for att in values_in_dataset_attributes:\n",
    "        values_in_dataset_indexes.append(get_index(att, attributes))\n",
    "        \n",
    "    for att in attributes:\n",
    "        if att not in sensitive_attributes + causal_reg_attributes + causal_class_attributes:\n",
    "            regular_indexes.append((get_index(att, attributes)))\n",
    "            \n",
    "    X_proba = X_proba.tolist() #list of every record in the dataset\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Removing records...\")\n",
    "    print(\"\")\n",
    "            \n",
    "    \n",
    "    #### PART2 - Record Removal [of DN / PP]\n",
    "    \n",
    "    # We're working with X_proba (list of records) and sensitive_dictionary (various informations about records)\n",
    "\n",
    "    \n",
    "    record_informations = []\n",
    "    # for every record removed, we append a \"record information\"\n",
    "    # a \"record information\" is a tuple of length == len(sensitive_attributes)\n",
    "    # each value in the tuple is the value the respective attribute had in the removed record\n",
    "    \n",
    "    # additionally, we'll make sure to remove the record in every list of records we have\n",
    "\n",
    "    for att in sensitive_attributes:\n",
    "        #### Removing DN\n",
    "        for val in sensitive_dict[att]['D']['values_list']:\n",
    "            if len(sensitive_dict[att]['D'][val]['N']) > sensitive_dict[att]['D'][val]['N_exp']:\n",
    "                to_remove = len(sensitive_dict[att]['D'][val]['N']) - sensitive_dict[att]['D'][val]['N_exp']\n",
    "                for record in sensitive_dict[att]['D'][val]['N'][:to_remove]:\n",
    "                    record_info = []\n",
    "                    for att2 in sensitive_attributes:\n",
    "                        index = get_index(att2, attributes)\n",
    "                        value = record[index]\n",
    "                        record_info.append(value)\n",
    "                        if att2 != att:\n",
    "                            for val2 in sensitive_dict[att2]['D']['values_list']:\n",
    "                                if record in sensitive_dict[att2]['D'][val2]['P']:\n",
    "                                    sensitive_dict[att2]['D'][val2]['P'].remove(record)\n",
    "                                if record in sensitive_dict[att2]['D'][val2]['N']:\n",
    "                                    sensitive_dict[att2]['D'][val2]['N'].remove(record)\n",
    "                            for val2 in sensitive_dict[att2]['P']['values_list']:\n",
    "                                if record in sensitive_dict[att2]['P'][val2]['P']:\n",
    "                                    sensitive_dict[att2]['P'][val2]['P'].remove(record)\n",
    "                                if record in sensitive_dict[att2]['P'][val2]['N']:\n",
    "                                    sensitive_dict[att2]['P'][val2]['N'].remove(record)                                    \n",
    "                    record_informations.append(tuple(record_info))\n",
    "                    X_proba.remove(record)\n",
    "                sensitive_dict[att]['D'][val]['N'] = sensitive_dict[att]['D'][val]['N'][to_remove:]\n",
    "        \n",
    "        ### Removing PP\n",
    "        for val in sensitive_dict[att]['P']['values_list']:\n",
    "            if len(sensitive_dict[att]['P'][val]['P']) > sensitive_dict[att]['P'][val]['P_exp']:\n",
    "                to_remove = len(sensitive_dict[att]['P'][val]['P']) - sensitive_dict[att]['P'][val]['P_exp']\n",
    "                for record in sensitive_dict[att]['P'][val]['P'][:to_remove]:\n",
    "                    record_info = []\n",
    "                    for att2 in sensitive_attributes:\n",
    "                        index = get_index(att2, attributes)\n",
    "                        value = record[index]\n",
    "                        record_info.append(value)\n",
    "                        if att2 != att:\n",
    "                            for val2 in sensitive_dict[att2]['D']['values_list']:\n",
    "                                if record in sensitive_dict[att2]['D'][val2]['P']:\n",
    "                                    sensitive_dict[att2]['D'][val2]['P'].remove(record)\n",
    "                                if record in sensitive_dict[att2]['D'][val2]['N']:\n",
    "                                    sensitive_dict[att2]['D'][val2]['N'].remove(record)\n",
    "                            for val2 in sensitive_dict[att2]['P']['values_list']:\n",
    "                                if record in sensitive_dict[att2]['P'][val2]['P']:\n",
    "                                    sensitive_dict[att2]['P'][val2]['P'].remove(record)\n",
    "                                if record in sensitive_dict[att2]['P'][val2]['N']:\n",
    "                                    sensitive_dict[att2]['P'][val2]['N'].remove(record)\n",
    "                    record_informations.append(tuple(record_info))\n",
    "                    X_proba.remove(record)\n",
    "                sensitive_dict[att]['P'][val]['P'] = sensitive_dict[att]['P'][val]['P'][to_remove:]\n",
    "\n",
    "    if len(record_informations) == 0:\n",
    "        print (\"No records removed! The dataset is already balanced\")\n",
    "        return\n",
    "        \n",
    "    print (\"Records removed:\", len(record_informations)) #each element (list) of record information => 1 removed record\n",
    "    print (\"\")\n",
    "    print (\"Current length of dataset: \", len(X_proba))  \n",
    "\n",
    "    #removed combinations of sensitive attributes:\n",
    "    common_combs = Counter(tuple(record_informations))\n",
    "    common_combs = list(OrderedDict(common_combs.most_common()))\n",
    "    \n",
    "    # the unique set of \"record information\", ordered by their frequency\n",
    "    # i.e. every unique combinations of sensitive attributes removed\n",
    "\n",
    "    \n",
    "    for comb in df_combinations:\n",
    "        if comb not in common_combs:\n",
    "            common_combs.append(comb)\n",
    "            \n",
    "    # additional unique combinations of sensitive attributes\n",
    "    # not among those removed\n",
    "    # we append them to the very end of the list => lower priority\n",
    "\n",
    "    \n",
    "    \n",
    "    ### PART3 - Combination Test [Getting the constraints]\n",
    "    \n",
    "    \n",
    "    constraints = []\n",
    "    # a constraint is a binary tuple\n",
    "    # tuple[0] = index of a feature; tuple[1] = value of the feature\n",
    "    # we'll make a constraint for the sensitive attribute(s) and the target variable\n",
    "    \n",
    "       \n",
    "    # \"N_cur\" => current length of the respective subset\n",
    "    for att in sensitive_attributes:\n",
    "        for val in sensitive_dict[att]['D']['values_list']:\n",
    "            sensitive_dict[att]['D'][val]['N_curr'] = len(sensitive_dict[att]['D'][val]['N'])\n",
    "            sensitive_dict[att]['D'][val]['P_curr'] = len(sensitive_dict[att]['D'][val]['P'])\n",
    "            DN_curr = sensitive_dict[att]['D'][val]['N_curr']\n",
    "            DN_exp = sensitive_dict[att]['D'][val]['N_exp']\n",
    "            DP_curr = sensitive_dict[att]['D'][val]['P_curr']\n",
    "            DP_exp = sensitive_dict[att]['D'][val]['P_exp']            \n",
    "        for val in sensitive_dict[att]['P']['values_list']:\n",
    "            sensitive_dict[att]['P'][val]['N_curr'] = len(sensitive_dict[att]['P'][val]['N'])\n",
    "            sensitive_dict[att]['P'][val]['P_curr'] = len(sensitive_dict[att]['P'][val]['P'])     \n",
    "            PN_curr = sensitive_dict[att]['P'][val]['N_curr']\n",
    "            PN_exp = sensitive_dict[att]['P'][val]['N_exp']\n",
    "            PP_curr = sensitive_dict[att]['P'][val]['P_curr']\n",
    "            PP_exp = sensitive_dict[att]['P'][val]['P_exp']            \n",
    "\n",
    "            \n",
    "            \n",
    "    # combinations of sensitive attributes values are ordered according to their frequency in common_combs\n",
    "    # higher frequency == higher priority\n",
    "    # for each value in the comb, we check if a new record with *that* value is needed\n",
    "    # (i.e, if N_curr < N_exp or P_curr < P_exp)\n",
    "    # if every value in the combination do pass the check (either for a negative record or a positive record)\n",
    "    # we'll make a constraint with those values (and the target variable)\n",
    "    # we repeat those steps as long as neither a negative or a positive record is needed\n",
    "    # then we'll try the next combination\n",
    "    # this way, before creating records with a less frequent combination of sens attributes values,\n",
    "    # we are sure we exhausted the more frequent combinations\n",
    "\n",
    "    for comb in common_combs:\n",
    "        while True:\n",
    "            starting_const_len = len(constraints)\n",
    "            check = True\n",
    "            ok_comb_neg = True\n",
    "            ok_comb_pos = True\n",
    "            constraint_neg = [(len(attributes), 0)]\n",
    "            constraint_pos = [(len(attributes), 1)]\n",
    "            for i in range(len(comb)):\n",
    "                att = sensitive_attributes[i]\n",
    "                val = comb[i]\n",
    "                \n",
    "                if val in sensitive_dict[att]['D']['values_list']:\n",
    "                    if sensitive_dict[att]['D'][val]['N_curr'] < sensitive_dict[att]['D'][val]['N_exp']:\n",
    "                        constraint_neg.append((get_index(att, attributes), val))\n",
    "                    else:\n",
    "                        ok_comb_neg = False \n",
    "                elif val in sensitive_dict[att]['P']['values_list']:\n",
    "                    if sensitive_dict[att]['P'][val]['N_curr'] < sensitive_dict[att]['P'][val]['N_exp']:\n",
    "                        constraint_neg.append((get_index(att, attributes), val))\n",
    "                    else:\n",
    "                        ok_comb_neg = False\n",
    "                        \n",
    "                if val in sensitive_dict[att]['D']['values_list']:\n",
    "                    if sensitive_dict[att]['D'][val]['P_curr'] < sensitive_dict[att]['D'][val]['P_exp']:\n",
    "                        constraint_pos.append((get_index(att, attributes), val))\n",
    "                    else:\n",
    "                        ok_comb_pos = False                        \n",
    "                elif val in sensitive_dict[att]['P']['values_list']:\n",
    "                    if sensitive_dict[att]['P'][val]['P_curr'] < sensitive_dict[att]['P'][val]['P_exp']:\n",
    "                        constraint_pos.append((get_index(att, attributes), val))\n",
    "                    else:\n",
    "                        ok_comb_pos = False   \n",
    "            \n",
    "            if ok_comb_neg == True:\n",
    "                constraints.append(tuple(constraint_neg))\n",
    "                for tup in constraint_neg[1:]:\n",
    "                    att = attributes[tup[0]]\n",
    "                    val = tup[1]\n",
    "                    if val in sensitive_dict[att]['D']['values_list']:\n",
    "                        sensitive_dict[att]['D'][val]['N_curr'] = sensitive_dict[att]['D'][val]['N_curr'] + 1\n",
    "                    elif val in sensitive_dict[att]['P']['values_list']:\n",
    "                        sensitive_dict[att]['P'][val]['N_curr'] = sensitive_dict[att]['P'][val]['N_curr'] + 1\n",
    "\n",
    "            if ok_comb_pos == True:\n",
    "                constraints.append(tuple(constraint_pos))\n",
    "                for tup in constraint_pos[1:]:\n",
    "                    att = attributes[tup[0]]\n",
    "                    val = tup[1]\n",
    "                    if val in sensitive_dict[att]['D']['values_list']:\n",
    "                        sensitive_dict[att]['D'][val]['P_curr'] = sensitive_dict[att]['D'][val]['P_curr'] + 1\n",
    "                    elif val in sensitive_dict[att]['P']['values_list']:\n",
    "                        sensitive_dict[att]['P'][val]['P_curr'] = sensitive_dict[att]['P'][val]['P_curr'] + 1\n",
    "            \n",
    "            if ok_comb_neg == False and ok_comb_pos == False:\n",
    "                break\n",
    "                \n",
    "            if len(constraints) == starting_const_len:\n",
    "                break\n",
    "\n",
    "    #print(\"XXX Printing stuff for debugging purposes...\")\n",
    "    #for att in sensitive_attributes:\n",
    "     #   for val in sensitive_dict[att]['D']['values_list']:\n",
    "      #      DN_curr = sensitive_dict[att]['D'][val]['N_curr']\n",
    "       #     DN_exp = sensitive_dict[att]['D'][val]['N_exp']\n",
    "        #    DP_curr = sensitive_dict[att]['D'][val]['P_curr']\n",
    "         #   DP_exp = sensitive_dict[att]['D'][val]['P_exp']            \n",
    "          #  print(att, val, \"DN_cur\", DN_curr, \"DN_exp\", DN_exp, \"DP_cur\", DP_curr, \"DP_exp\", DP_exp)\n",
    "        #for val in sensitive_dict[att]['P']['values_list']: \n",
    "         #   PN_curr = sensitive_dict[att]['P'][val]['N_curr']\n",
    "          #  PN_exp = sensitive_dict[att]['P'][val]['N_exp']\n",
    "           # PP_curr = sensitive_dict[att]['P'][val]['P_curr']\n",
    "            #PP_exp = sensitive_dict[att]['P'][val]['P_exp']            \n",
    "            #print(att, val, \"PN_cur\", PN_curr, \"PN_exp\", PN_exp, \"PP_cur\", PP_curr, \"PP_exp\", PP_exp)         \n",
    "\n",
    "    ### PART4 - Creating new records using the constraints to balance the dataset\n",
    "    \n",
    "    constraints = Counter(tuple(constraints))\n",
    "            \n",
    "    for const in constraints.keys():\n",
    "        \n",
    "        subset = og_df.copy() #subgroup of records with const values (e.g.: Black, Male, Positive)\n",
    "        for tup in const:\n",
    "            subset = subset[subset[subset.columns[tup[0]]] == tup[1]]\n",
    "            \n",
    "        if len(subset) > 0:\n",
    "            kmedoids = KMedoids(n_clusters=1, random_state=42).fit(subset)\n",
    "            medoid = kmedoids.cluster_centers_[0] #medoid of subgroup\n",
    "        else:\n",
    "            medoid = [None]\n",
    "        \n",
    "\n",
    "        #we use a GA for every const [eg. Black, Male, Positve] and each associated number [e.g. 5]\n",
    "        #e.g., we create 5 records following the consts\n",
    "        #if we have 20 unique consts, we use the GA 20 times\n",
    "        \n",
    "        new_records = GA(values, const, constraints[const], forest, medoid, \n",
    "                         values_in_dataset_indexes, discrete_indexes, regular_indexes, causal_reg, \n",
    "                         causal_class, mode, ds)\n",
    "\n",
    "        for all_records in new_records:\n",
    "            for record in all_records:\n",
    "                target = const[0][1]\n",
    "                for tup in const[1:]:\n",
    "                    att = attributes[tup[0]]\n",
    "                    val = tup[1]\n",
    "                    if val in sensitive_dict[att]['D']['values_list']:\n",
    "                        if target == 0: \n",
    "                            sensitive_dict[att]['D'][val]['N'].append(record)\n",
    "                        else:\n",
    "                            sensitive_dict[att]['D'][val]['P'].append(record)\n",
    "                    elif val in sensitive_dict[att]['P']['values_list']:\n",
    "                        if target == 0: \n",
    "                            sensitive_dict[att]['P'][val]['N'].append(record)\n",
    "                        else:\n",
    "                            sensitive_dict[att]['P'][val]['P'].append(record)\n",
    "                    else:\n",
    "                        print (\"ERROR! ERROR!\")\n",
    "                        print (\"Value\", val, \"shouldn't exist for attribute\", att)\n",
    "                        \n",
    "                X_proba.append(record) #balanced dataset (OG + Syntethic)\n",
    "                genetic_data.append(record) #other dataset with ONLY synthetic data\n",
    "       \n",
    "    print (\"=== NEW DATASET ===\")\n",
    "    final_df = pd.DataFrame.from_records(X_proba)\n",
    "    final_df.columns = attributes + [class_name]\n",
    "    \n",
    "    genetic_df = pd.DataFrame.from_records(genetic_data)\n",
    "    genetic_df.columns = attributes + [class_name]\n",
    "    \n",
    "    \n",
    "    get_discrimination (final_df, sensitive_attributes, class_name)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"OG dataset length:\", len(df))\n",
    "    print(\"Records generated:\", len(genetic_df))\n",
    "    print(\"New dataset length:\", len(final_df))\n",
    "    print(\"Time:\", (time.time() - start_time))\n",
    "    \n",
    "    return final_df, genetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff932f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "german = pd.read_json(\"german_train.json\")\n",
    "\n",
    "german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9443398",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_attributes = ['Sex', 'Age']\n",
    "class_name = 'Risk'\n",
    "\n",
    "causal_reg = []\n",
    "causal_class = [[['Job'], 'Housing']] \n",
    "#multiple independent features are supported, e.g. [[['Job', 'Credit amount'], 'Housing']]\n",
    "\n",
    "discrete_attributes = []\n",
    "values_in_dataset_attributes = ['Job', 'Housing', 'Saving accounts', 'Checking account', 'Credit amount', 'Duration', 'Purpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee411bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fair, df_genetic = main(german, sensitive_attributes, class_name, #<- must be included\n",
    "                           causal_reg, causal_class, #<- causal relations\n",
    "                           discrete_attributes, values_in_dataset_attributes, #<- how to handle the various features\n",
    "                          'Outlier', 'Fixed') #<- fitness mode, datatased values mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b79c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
